Install latest Virtualbox (version 6.1.10r138449)
Install latest vagrant (Version 2.2.9 at the time)
Start by doing vagrant up. Vagrantfile is modified such that the memory size is 2048, cpus is 2 and install docker by shell commands and swap is turned off.
This is what happens via setup.sh in virtual machine (vagrant up will do this for you)
	- yum update
	- Docker installation:
		# (Install Docker CE)
		## Set up the repository
		### Install required packages
		yum install -y yum-utils device-mapper-persistent-data lvm2

		## Add the Docker repository
		yum-config-manager --add-repo \
		  https://download.docker.com/linux/centos/docker-ce.repo


		# Install Docker CE, version 19.03 is the recommended one via official kubeadm docs but 1.13.1, 17.03, 17.06, 17.09, 18.06 and 18.09 are known to work as well
		yum update -y && yum install -y \
		  containerd.io-1.2.13 \
		  docker-ce-19.03.11 \
		  docker-ce-cli-19.03.11

		## Create /etc/docker
		mkdir /etc/docker

		# Set up the Docker daemon
		cat > /etc/docker/daemon.json <<EOF
		{
		  "exec-opts": ["native.cgroupdriver=systemd"],
		  "log-driver": "json-file",
		  "log-opts": {
		    "max-size": "100m"
		  },
		  "storage-driver": "overlay2",
		  "storage-opts": [
		    "overlay2.override_kernel_check=true"
		  ]
		}
		EOF

		mkdir -p /etc/systemd/system/docker.service.d

		# Restart Docker
		systemctl daemon-reload
		systemctl restart docker

		# If you want the docker service to start on boot, run the following command:
		sudo systemctl enable docker


	- installation of these packages on your machines: kubeadm, kubelet, kubectl
	- following commands (applies for CentOS, RHEL or Fedora)(https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)
		cat <<EOF > /etc/yum.repos.d/kubernetes.repo
		[kubernetes]
		name=Kubernetes
		baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
		enabled=1
		gpgcheck=1
		repo_gpgcheck=1
		gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
		exclude=kubelet kubeadm kubectl
		EOF

		# Set SELinux in permissive mode (effectively disabling it)
		setenforce 0
		sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

		yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

		systemctl enable --now kubelet

	- When using Docker, kubeadm will automatically detect the cgroup driver for the kubelet and set it in the /var/lib/kubelet/config.yaml file during runtime, so if you are using any other CRI check online docs.
	- Restarting the kubelet is required:
		systemctl daemon-reload
		systemctl restart kubelet

######### Spawning 2 VM's with setup completed and additional stuff to create cluster ########
	- vagrant up will spawn two virtual machines
		- k8s-head supposed to be the master of the cluster with ip 192.168.205.10
		- k8s-node-1 supposed to be the worker of the cluster with ip 192.168.205.11

	- set hostname( can be moved to vagrant provision)
		[root@k8s-head vagrant]# HOST_NAME=$(hostname -s) 

	- set IP address variable at k8s-head (this can be moved to vagrant provision)
		# IP_ADDR=192.168.205.10

	- kubeadm init with the following instructions
		[root@k8s-head vagrant]# kubeadm init --apiserver-advertise-address=$IP_ADDR --apiserver-cert-extra-sans=$IP_ADDR --node-name $HOST_NAME --pod-network-cidr=192.168.0.0/16

	- To start using your cluster, you need to run the following as a regular user:
		mkdir -p $HOME/.kube
		sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
		sudo chown $(id -u):$(id -g) $HOME/.kube/config

	- The next step is going to be to deploy a cluster so before that we should add the node 'k8s-node-1' to this cluster ( the command to add new nodes will be available when the cluster creation at master completes make sure to copy it from there)
		[root@k8s-node-1 vagrant]# kubeadm join 192.168.205.10:6443 --token yuc3ou.dyoai0yris8wszn9 \
		>     --discovery-token-ca-cert-hash sha256:78255514d13c5bf7f1bb3f8a3511fb1574e57c37fba984d7788a1d758c87446a

		OUTPUT

		W0627 13:03:32.561915   13838 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
		[preflight] Running pre-flight checks
		[preflight] Reading configuration from the cluster...
		[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
		[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.18" ConfigMap in the kube-system namespace
		[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
		[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
		[kubelet-start] Starting the kubelet
		[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

		This node has joined the cluster:
		* Certificate signing request was sent to apiserver and a response was received.
		* The Kubelet was informed of the new secure connection details.

		Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

	- Now on the master we deploy the calico networking pod config
		kubectl apply -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml


######### After adding one more node and deploying calico pod network #########
	-	[vagrant@k8s-head ~]$ kubectl get pods --all-namespaces -o wide
		NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE         NOMINATED NODE   READINESS GATES
		kube-system   calico-kube-controllers-76d4774d89-5qqz2   0/1     Running   0          3m55s   192.168.193.193   k8s-head     <none>           <none>
		kube-system   calico-node-26v4w                          1/1     Running   0          3m56s   10.0.2.15         k8s-head     <none>           <none>
		kube-system   calico-node-bdf9k                          1/1     Running   0          3m56s   10.0.2.15         k8s-node-1   <none>           <none>
		kube-system   coredns-66bff467f8-l298d                   1/1     Running   0          19m     192.168.193.194   k8s-head     <none>           <none>
		kube-system   coredns-66bff467f8-rvspb                   0/1     Running   0          19m     192.168.109.65    k8s-node-1   <none>           <none>
		kube-system   etcd-k8s-head                              1/1     Running   0          20m     10.0.2.15         k8s-head     <none>           <none>
		kube-system   kube-apiserver-k8s-head                    1/1     Running   0          20m     10.0.2.15         k8s-head     <none>           <none>
		kube-system   kube-controller-manager-k8s-head           1/1     Running   2          20m     10.0.2.15         k8s-head     <none>           <none>
		kube-system   kube-proxy-59bld                           1/1     Running   0          15m     10.0.2.15         k8s-node-1   <none>           <none>
		kube-system   kube-proxy-mtl6r                           1/1     Running   0          19m     10.0.2.15         k8s-head     <none>           <none>
		kube-system   kube-scheduler-k8s-head                    1/1     Running   2          20m     10.0.2.15         k8s-head     <none>           <none>
		[vagrant@k8s-head ~]$ kubectl get nodes
		NAME         STATUS   ROLES    AGE   VERSION
		k8s-head     Ready    master   20m   v1.18.5
		k8s-node-1   Ready    <none>   15m   v1.18.5
		[vagrant@k8s-head ~]$ 
