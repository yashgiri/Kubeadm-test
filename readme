Install latest vagrant
Start a vagrant vm by doing vagrant init and making changes to the Vagrantfile so the memory size is 2048, cpus is 2 and install docker by shell commands and swap is turned off.
This is what happens via setup.sh in virtual machine
	- yum update
	- Docker installation:
		# (Install Docker CE)
		## Set up the repository
		### Install required packages
		yum install -y yum-utils device-mapper-persistent-data lvm2

		## Add the Docker repository
		yum-config-manager --add-repo \
		  https://download.docker.com/linux/centos/docker-ce.repo


		# Install Docker CE, version 19.03 is the recommended one via official kubeadm docs but 1.13.1, 17.03, 17.06, 17.09, 18.06 and 18.09 are known to work as well
		yum update -y && yum install -y \
		  containerd.io-1.2.13 \
		  docker-ce-19.03.11 \
		  docker-ce-cli-19.03.11

		## Create /etc/docker
		mkdir /etc/docker

		# Set up the Docker daemon
		cat > /etc/docker/daemon.json <<EOF
		{
		  "exec-opts": ["native.cgroupdriver=systemd"],
		  "log-driver": "json-file",
		  "log-opts": {
		    "max-size": "100m"
		  },
		  "storage-driver": "overlay2",
		  "storage-opts": [
		    "overlay2.override_kernel_check=true"
		  ]
		}
		EOF

		mkdir -p /etc/systemd/system/docker.service.d

		# Restart Docker
		systemctl daemon-reload
		systemctl restart docker

		# If you want the docker service to start on boot, run the following command:
		sudo systemctl enable docker


	- installation of these packages on your machines: kubeadm, kubelet, kubectl
	- following commands (applies for CentOS, RHEL or Fedora)(https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)
		cat <<EOF > /etc/yum.repos.d/kubernetes.repo
		[kubernetes]
		name=Kubernetes
		baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
		enabled=1
		gpgcheck=1
		repo_gpgcheck=1
		gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
		exclude=kubelet kubeadm kubectl
		EOF

		# Set SELinux in permissive mode (effectively disabling it)
		setenforce 0
		sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

		yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

		systemctl enable --now kubelet

	- When using Docker, kubeadm will automatically detect the cgroup driver for the kubelet and set it in the /var/lib/kubelet/config.yaml file during runtime, so if you are using any other CRI check online docs.
	- Restarting the kubelet is required:
		systemctl daemon-reload
		systemctl restart kubelet

########## Installation is done directly during vagrant up, move to Install a single control-plane Kubernetes cluster ###########
	- ssh into vm
	- kubeadm init (look at online docs for modifications to your cluster)

		Your Kubernetes control-plane has initialized successfully!

	- To start using your cluster, you need to run the following as a regular user:

		mkdir -p $HOME/.kube
		sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
		sudo chown $(id -u):$(id -g) $HOME/.kube/config

	- let us check the cluster status 

		Kubernetes master is running at https://10.0.2.15:6443
		KubeDNS is running at https://10.0.2.15:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

		To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

	- You should now deploy a pod network to the cluster.
	
		Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
		https://kubernetes.io/docs/concepts/cluster-administration/addons/

		Then you can join any number of worker nodes by running the following on each as root:

		kubeadm join 10.0.2.15:6443 --token 7nqd9k.8bq7qi0cr2bdsl2w \
		    --discovery-token-ca-cert-hash sha256:ed0175dfabd807d9e378d86b8211b25f500ff800b387fe39f0afb7ee88b81ee9

	- i tried calico
		kubectl apply -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml

		[vagrant@localhost ~]$ kubectl get pods --all-namespaces
		NAMESPACE     NAME                                            READY   STATUS    RESTARTS   AGE
		kube-system   calico-kube-controllers-76d4774d89-fcmbt        1/1     Running   0          7m28s
		kube-system   calico-node-gktc7                               1/1     Running   0          7m28s
		kube-system   coredns-66bff467f8-s9g5n                        1/1     Running   0          15m
		kube-system   coredns-66bff467f8-wgts5                        1/1     Running   0          15m
		kube-system   etcd-localhost.localdomain                      1/1     Running   0          15m
		kube-system   kube-apiserver-localhost.localdomain            1/1     Running   0          15m
		kube-system   kube-controller-manager-localhost.localdomain   1/1     Running   0          15m
		kube-system   kube-proxy-nwfk6                                1/1     Running   0          15m
		kube-system   kube-scheduler-localhost.localdomain            1/1     Running   0          15m


######### After adding one more node and deploying calico pod network #########
	-	[vagrant@k8s-head ~]$ kubectl get pods --all-namespaces -o wide
		NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE         NOMINATED NODE   READINESS GATES
		kube-system   calico-kube-controllers-76d4774d89-5qqz2   0/1     Running   0          3m55s   192.168.193.193   k8s-head     <none>           <none>
		kube-system   calico-node-26v4w                          1/1     Running   0          3m56s   10.0.2.15         k8s-head     <none>           <none>
		kube-system   calico-node-bdf9k                          1/1     Running   0          3m56s   10.0.2.15         k8s-node-1   <none>           <none>
		kube-system   coredns-66bff467f8-l298d                   1/1     Running   0          19m     192.168.193.194   k8s-head     <none>           <none>
		kube-system   coredns-66bff467f8-rvspb                   0/1     Running   0          19m     192.168.109.65    k8s-node-1   <none>           <none>
		kube-system   etcd-k8s-head                              1/1     Running   0          20m     10.0.2.15         k8s-head     <none>           <none>
		kube-system   kube-apiserver-k8s-head                    1/1     Running   0          20m     10.0.2.15         k8s-head     <none>           <none>
		kube-system   kube-controller-manager-k8s-head           1/1     Running   2          20m     10.0.2.15         k8s-head     <none>           <none>
		kube-system   kube-proxy-59bld                           1/1     Running   0          15m     10.0.2.15         k8s-node-1   <none>           <none>
		kube-system   kube-proxy-mtl6r                           1/1     Running   0          19m     10.0.2.15         k8s-head     <none>           <none>
		kube-system   kube-scheduler-k8s-head                    1/1     Running   2          20m     10.0.2.15         k8s-head     <none>           <none>
		[vagrant@k8s-head ~]$ kubectl get nodes
		NAME         STATUS   ROLES    AGE   VERSION
		k8s-head     Ready    master   20m   v1.18.5
		k8s-node-1   Ready    <none>   15m   v1.18.5
		[vagrant@k8s-head ~]$ 
